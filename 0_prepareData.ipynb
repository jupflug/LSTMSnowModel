{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f5c26-4fa7-4f04-9a5e-573e97efb320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573697b-7eb6-4373-9786-8acf2106e357",
   "metadata": {},
   "source": [
    "### Define user parameters and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1caf7-e1b2-4dae-b846-a6562a65426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years to train over\n",
    "training_years = np.arange(2004,2020)\n",
    "\n",
    "# directory containing the tiled training data\n",
    "data_direc = '/direcc/jpflug/ML_layers/WUS_tiles/'\n",
    "\n",
    "# range of dates to check for zero snow -- filtering ephemeral snowpack\n",
    "# day = 0 is September 1\n",
    "filt_range = [150,180]\n",
    "\n",
    "# number of days in the data year\n",
    "# keep this lower than 360\n",
    "noDays = 330\n",
    "\n",
    "# specify the degree tiles to train the model over\n",
    "# this should be a CSV file of form (x_coord,y_coord) of the lower left coordinate\n",
    "extents_file = data_direc+'degreeTilesWUS_all.csv'\n",
    "\n",
    "# specify the maximum SWE limit to filter bad UCLA grid cells\n",
    "lim = 3.0\n",
    "\n",
    "# specify where to output the trained model outputs\n",
    "modelOutputs_direc = data_direc+'temp/trial8/'\n",
    "\n",
    "# specify whether or not to run and save outputs\n",
    "# save_outputs = 0: numpy-load data, save_outputs = 1: process tile data\n",
    "save_outputs = 0\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(modelOutputs_direc, exist_ok=True)\n",
    "\n",
    "# list of SNOTEL stations to exclude from the taining data\n",
    "station_list = '/direcc/jpflug/STN_data/SNOTEL_array.csv'\n",
    "\n",
    "######### don't change this. Only used to save the parameters to a text file for reference\n",
    "source_code = inspect.getsource(inspect.currentframe())\n",
    "source_code_lines = source_code.split('\\n')\n",
    "start_line = source_code_lines.index('# Define the cell content') + 1\n",
    "relevant_code = '\\n'.join(source_code_lines[start_line:-5]) \n",
    "# Save the source code to a file\n",
    "output_file_path = modelOutputs_direc + 'model_parameters.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(relevant_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92a99d-7050-4dcf-ac7a-8d83e464f72c",
   "metadata": {},
   "source": [
    "### Script functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f284cf0-bb05-42eb-88aa-b4b6a48aaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask ephemeral snow and bad data cells\n",
    "def filter_cells(ds,filt_range,lim):\n",
    "    # instantiate\n",
    "    ds_mask = ds.copy()\n",
    "    ds_mask2 = ds.copy()\n",
    "    # ID cells where the first date already has snow\n",
    "    condition = ds_mask[0, :] > 0.02\n",
    "    ds_mask[filt_range[0],condition] = -1\n",
    "    # ID cells with ephemeral snow\n",
    "    ds_mask = ds_mask[filt_range[0]:filt_range[1],:]\n",
    "    ds_mask[ds_mask < 0.02] = -1\n",
    "    ds_mask[ds_mask > 0] = 0\n",
    "    # sum flags to identify all bad cells\n",
    "    ds_mask = np.nansum(ds_mask,axis=0)\n",
    "    # filter unrealistically high SWE\n",
    "    ds_mask2[ds_mask2 <= lim] = 0\n",
    "    ds_mask2[ds_mask2 > lim] = -1\n",
    "    ds_mask2 = np.nansum(ds_mask2,axis=0)\n",
    "    ds_mask = ds_mask + ds_mask2\n",
    "    return ds_mask\n",
    "\n",
    "# remove cells overlapping snow point stations\n",
    "def filter_stations(lonn,latt,stations_x,stations_y,differ_lim):\n",
    "    for x,y in zip(stations_x,stations_y):\n",
    "        differ = np.sqrt(((lonn-x)**2)+((latt-y)**2))\n",
    "        latt[differ <= differ_lim] = np.nan\n",
    "        lonn[differ <= differ_lim] = np.nan\n",
    "    return lonn,latt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f16d31-f53e-49f0-8249-35f8951a5e8c",
   "metadata": {},
   "source": [
    "### Read in and prepare the SWE training targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e1b74-6396-415b-9e38-83097bb81353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the pandas array of degree tiles to read in\n",
    "extents = pd.read_csv(extents_file,header=None)\n",
    "\n",
    "# if preparing the data (not saved from a previous iteration of this script)\n",
    "if save_outputs:\n",
    "    # load the snow station locations\n",
    "    stations = pd.read_csv(station_list)\n",
    "    stations_x = stations['x'].values\n",
    "    stations_y = stations['y'].values\n",
    "    del stations\n",
    "    \n",
    "    # load the SWE data and make a mask array to use for the other datasets\n",
    "    # flag to determine whether to concatenate data, or first time through the loop\n",
    "    flag = 0\n",
    "    # loop through the tiles\n",
    "    for index, row in extents.iterrows():\n",
    "        # lower-left coordinate of the tile\n",
    "        ll_x = row[0]\n",
    "        ll_y = row[1]\n",
    "        # loop through the training years\n",
    "        for yearCount,year in enumerate(training_years):\n",
    "            print(year,ll_x,ll_y)\n",
    "\n",
    "            # read in the xarray tile for the corresponding year and lat/lon tile\n",
    "            ds = xr.open_dataset(data_direc+'UCLASWEtile_'+str(year)+'_'+str(ll_y)+'_'+str(ll_x)+'.nc')\n",
    "\n",
    "            # prepare the spatial data arrays for lat/lon and mask grid cells with snow stations\n",
    "            if yearCount == 0:\n",
    "                latVals = ds['y'].values\n",
    "                lonVals = ds['x'].values\n",
    "                # determine the grid size\n",
    "                differ_lim = (np.mean(np.diff(latVals))/2)**2\n",
    "                differ_lim = differ_lim + (np.mean(np.diff(lonVals))/2)**2\n",
    "                differ_lim = np.sqrt(differ_lim)\n",
    "                lonVals,latVals = np.meshgrid(lonVals,latVals)\n",
    "                # flatten the spatial component of the data\n",
    "                latVals = latVals.reshape((-1))\n",
    "                lonVals = lonVals.reshape((-1))\n",
    "                # filter stations\n",
    "                lonVals,latVals = filter_stations(lonVals,latVals,stations_x,stations_y,differ_lim)\n",
    "               \n",
    "            # flatten the data\n",
    "            ds = ds['SWE'].values.reshape((noDays, -1)).astype('float16')\n",
    "\n",
    "            # mask ephemeral snow and bad data cells\n",
    "            ds_mask = filter_cells(ds,filt_range,lim) \n",
    "            ds_mask[np.isnan(lonVals)] = -1\n",
    "            pctg = np.round(len(ds_mask[ds_mask < 0])/len(ds_mask)*100)\n",
    "            print('percentage of data filtered:'+str(pctg))\n",
    "            \n",
    "            # create an array for the year\n",
    "            yearVals = np.ones(ds.shape[1])*year\n",
    "            yearVals = yearVals.astype(int)\n",
    "\n",
    "            # concatenate the data arrays\n",
    "            if flag == 0:\n",
    "                dsSWE = ds\n",
    "                dsMASK = ds_mask\n",
    "                dsLON = lonVals\n",
    "                dsLAT = latVals\n",
    "                dsYR = yearVals\n",
    "                flag = 1\n",
    "            else:\n",
    "                dsSWE = np.hstack((dsSWE,ds))\n",
    "                dsMASK = np.hstack((dsMASK,ds_mask))\n",
    "                dsLON = np.hstack((dsLON,lonVals))\n",
    "                dsLAT = np.hstack((dsLAT,latVals))\n",
    "                dsYR = np.hstack((dsYR,yearVals))\n",
    "\n",
    "            # delete variables to save memory\n",
    "            del ds,ds_mask,pctg\n",
    "\n",
    "    # filter based on the SWE mask\n",
    "    dsMASK = np.unique(np.where(dsMASK == 0))\n",
    "    dsSWE = dsSWE[:,dsMASK]\n",
    "    dsLON = dsLON[dsMASK]\n",
    "    dsLAT = dsLAT[dsMASK]\n",
    "    dsYR = dsYR[dsMASK]\n",
    "\n",
    "    # save the data\n",
    "    np.save(modelOutputs_direc+'SWEdata_masked.npy',dsSWE)\n",
    "    np.save(modelOutputs_direc+'LONdata_masked.npy',dsLON)\n",
    "    np.save(modelOutputs_direc+'LATdata_masked.npy',dsLAT)\n",
    "    np.save(modelOutputs_direc+'maskArray.npy',dsMASK)\n",
    "    np.save(modelOutputs_direc+'YEARdata_masked.npy',dsYR)\n",
    "    \n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    dsSWE = np.load(modelOutputs_direc+'SWEdata_masked.npy')\n",
    "    dsLON = np.load(modelOutputs_direc+'LONdata_masked.npy')\n",
    "    dsLAT = np.load(modelOutputs_direc+'LATdata_masked.npy')\n",
    "    dsMASK = np.load(modelOutputs_direc+'maskArray.npy')\n",
    "    dsYR = np.load(modelOutputs_direc+'YEARdata_masked.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8f6e1-de32-4b33-a5e9-a165019c6e72",
   "metadata": {},
   "source": [
    "### Read in and prepare MODIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d09ee0-8c93-4724-82c5-786c792c313d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if preparing the data (not saved from a previous iteration of this script)\n",
    "if save_outputs:\n",
    "    # flag to determine whether to concatenate data, or first time through the loop\n",
    "    flag = 0\n",
    "    # loop through the tiles\n",
    "    for index, row in extents.iterrows():\n",
    "        ll_x = row[0]\n",
    "        ll_y = row[1]\n",
    "        # loop through the training years\n",
    "        for yearCount,year in enumerate(training_years):\n",
    "            print(year,ll_x,ll_y)\n",
    "\n",
    "            # read in the xarray tile for the corresponding year and lat/lon tile\n",
    "            ds = xr.open_dataset(data_direc+'MOD10Atile_'+str(year)+'_'+str(ll_y)+'_'+str(ll_x)+'.nc')\n",
    "            ds = ds['SCA'].values.reshape((noDays, -1)).astype('float16')\n",
    "\n",
    "            # concatenate the data arrays\n",
    "            if flag == 0:\n",
    "                dsSCF = ds\n",
    "                flag = 1\n",
    "            else:\n",
    "                dsSCF = np.hstack((dsSCF,ds))\n",
    "\n",
    "            del ds\n",
    "        \n",
    "    # filter based on the SWE mask\n",
    "    dsSCF = dsSCF[:,dsMASK]\n",
    "    np.save(modelOutputs_direc+'SCFdata_masked.npy',dsSCF)\n",
    "    \n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    dsSCF = np.load(modelOutputs_direc+'SCFdata_masked.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c73b3-1999-465d-a9c0-7760cd762290",
   "metadata": {},
   "source": [
    "### Read in and prepare the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc3bcd-16ae-40db-a5dd-cde6f1f62a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if preparing the data (not saved from a previous iteration of this script)\n",
    "if save_outputs:\n",
    "    # flag to determine whether to concatenate data, or first time through the loop\n",
    "    flag = 0\n",
    "    # loop through the tiles\n",
    "    for index, row in extents.iterrows():\n",
    "        ll_x = row[0]\n",
    "        ll_y = row[1]\n",
    "        # loop through the training years\n",
    "        for yearCount,year in enumerate(training_years):\n",
    "            print(year,ll_x,ll_y)\n",
    "\n",
    "            # read in the xarray tile for the corresponding year and lat/lon tile\n",
    "            ds = xr.open_dataset(data_direc+'LISTtile_'+str(year)+'_'+str(ll_y)+'_'+str(ll_x)+'.nc')\n",
    "            ds = ds['T'].values.reshape((noDays, -1)).astype('float16')\n",
    "\n",
    "            # concatenate the data arrays\n",
    "            if flag == 0:\n",
    "                dsT = ds\n",
    "                flag = 1\n",
    "            else:\n",
    "                dsT = np.hstack((dsT,ds))\n",
    "\n",
    "            del ds\n",
    "        \n",
    "    # filter based on the SWE mask\n",
    "    dsT = dsT[:,dsMASK]\n",
    "    np.save(modelOutputs_direc+'Tdata_masked.npy',dsT)\n",
    "\n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    dsT = np.load(modelOutputs_direc+'Tdata_masked.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6064ab2-d066-456c-8885-8cc25c0b3fdf",
   "metadata": {},
   "source": [
    "### Read in and prepare the precipitation climatology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855badd-47c4-4e96-ba8a-ff85af32c7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if preparing the data (not saved from a previous iteration of this script)\n",
    "if save_outputs:\n",
    "    # flag to determine whether to concatenate data, or first time through the loop\n",
    "    flag = 0\n",
    "    # loop through the tiles\n",
    "    for index, row in extents.iterrows():\n",
    "        ll_x = row[0]\n",
    "        ll_y = row[1]\n",
    "        \n",
    "        # load the precipitation climatology for this tile\n",
    "        ds = xr.open_dataset(data_direc+'LISPclimo_'+str(ll_y)+'_'+str(ll_x)+'.nc')\n",
    "        ds = ds['P'].values.reshape((noDays, -1)).astype('float16')\n",
    "        \n",
    "        # loop through the training years\n",
    "        for yearCount,year in enumerate(training_years):\n",
    "            print(year,ll_x,ll_y)\n",
    "\n",
    "            # concatenate the data arrays\n",
    "            if flag == 0:\n",
    "                dsP = ds\n",
    "                flag = 1\n",
    "            else:\n",
    "                dsP = np.hstack((dsP,ds))\n",
    "        \n",
    "    # filter based on the SWE mask\n",
    "    dsP = dsP[:,dsMASK]\n",
    "    np.save(modelOutputs_direc+'PCLIMdata_masked.npy',dsP)\n",
    "    \n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    dsP = np.load(modelOutputs_direc+'PCLIMdata_masked.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec0913-ced0-4d97-8d48-c82d7ed78efe",
   "metadata": {},
   "source": [
    "### Perform the snow cover post-processed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d2ede-93fc-44db-97aa-10b04b1f4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if preparing the data (not saved from a previous iteration of this script)\n",
    "if save_outputs:\n",
    "    # do the strictly-accumulating snow cover post-processing\n",
    "    SCFaccum = dsSCF.copy()\n",
    "    SCFaccum[np.isnan(SCFaccum)] = -9999\n",
    "    \n",
    "    # calculate the index of snow cover initialization\n",
    "    idx_start = np.nanargmax(SCFaccum > 0.01, axis=0)\n",
    "    for idxCount,idxx in enumerate(idx_start):\n",
    "        SCFaccum[:idxx,idxCount] = -9999\n",
    "        \n",
    "    # determine max and slope to max\n",
    "    idx_end = np.nanargmax(SCFaccum, axis=0)\n",
    "    max_vals = np.array([SCFaccum[A,counter] for counter,A in enumerate(idx_end)])\n",
    "    slope = max_vals/(idx_end-idx_start)\n",
    "\n",
    "    # calcuate the post processed quasi snow accumulation\n",
    "    for i in np.arange(slope.shape[0]):\n",
    "        SCFaccum[idx_start[i]:idx_end[i],i] = (np.arange(idx_start[i],idx_end[i])*slope[i])-(idx_start[i]*slope[i])\n",
    "    np.save(modelOutputs_direc+'SCFaccum_masked.npy',SCFaccum)\n",
    "    \n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    SCFaccum = np.load(modelOutputs_direc+'SCFaccum_masked.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc634eae-652f-4343-b580-cb4750d2f9ce",
   "metadata": {},
   "source": [
    "### Final data prep: ID nodata, normalize, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188ba0d-883a-4b5e-8c79-fafdc86545db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the lingering no data values\n",
    "print('finding nodata SWE...')\n",
    "nan_locs_swe = np.unique(np.where(np.isnan(dsSWE))[1])\n",
    "print('finding nodata Temp...')\n",
    "nan_locs_T = np.unique(np.where(np.isnan(dsT))[1])\n",
    "print('finding nodata Precip...')\n",
    "nan_locs_P = np.unique(np.where(np.isnan(dsP))[1])\n",
    "print('combining nodata indices...')\n",
    "nan_locs = np.hstack((nan_locs_swe,nan_locs_T,nan_locs_P))\n",
    "filt = np.arange(dsSWE.shape[1])\n",
    "mask = np.isin(filt,nan_locs,invert=True)\n",
    "filt = filt[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6157905-842d-427f-b74c-42209dc6004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the layers and fill the nodata values for snow cover\n",
    "dsSWE = dsSWE[:,filt]\n",
    "dsSCF = dsSCF[:,filt]\n",
    "SCFaccum = SCFaccum[:,filt]\n",
    "dsT = dsT[:,filt]\n",
    "dsP = dsP[:,filt]\n",
    "print('filtered data complete')\n",
    "\n",
    "print('normalizing SWE...')\n",
    "dsSWE = dsSWE/lim\n",
    "print(np.max(dsSWE))\n",
    "\n",
    "print('normalizing temperature...')\n",
    "min_T = np.min(dsT)\n",
    "max_T = np.max(dsT)\n",
    "dsT = (dsT-min_T)/(max_T-min_T)\n",
    "print(min_T,max_T)\n",
    "\n",
    "print('normalizing precipitation...')\n",
    "max_P = np.max(dsP)\n",
    "dsP = dsP/max_P\n",
    "print('0.0',max_P)\n",
    "\n",
    "print('setting nodata for masked layers...')\n",
    "dsSCF[np.isnan(dsSCF)] = -9999\n",
    "dsSCF[dsSCF < 0] = -9999\n",
    "SCFaccum[np.isnan(SCFaccum)] = -9999\n",
    "SCFaccum[SCFaccum < 0] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174adf83-55e7-4033-901f-4a24d3209271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the normalization bounds\n",
    "if save_outputs:\n",
    "    # order of form: SWEmax,SWEanom_min,SWEanom_max,min_T,max_T,max_P\n",
    "    # norm_bounds = np.array([lim,min_anom,max_anom,min_T,max_T,max_P])\n",
    "    norm_bounds = np.array([lim,0,0,min_T,max_T,max_P])\n",
    "    np.save(modelOutputs_direc+'normBounds.npy',norm_bounds)\n",
    "    \n",
    "# if the data has already been prepared, and now just reading in\n",
    "else:\n",
    "    norm_bounds = np.load(modelOutputs_direc+'normBounds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75628cc2-84f8-4e41-99e9-a549ce9a4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot function for visualization, if you want\n",
    "locc = random.randint(0,SCFaccum.shape[1])\n",
    "\n",
    "fg,ax = plt.subplots()\n",
    "ax.plot(np.arange(noDays),dsSWE[:,locc],'-k')\n",
    "ax.plot(np.arange(noDays),dsSWE_anom[:,locc],'--k')\n",
    "ax.scatter(np.arange(noDays),dsSCF[:,locc])\n",
    "ax.scatter(np.arange(noDays),SCFaccum[:,locc])\n",
    "ax.plot(np.arange(noDays),dsT[:,locc])\n",
    "ax.plot(np.arange(noDays),dsP[:,locc])\n",
    "# strng = str(dsYR[locc])+', '+str(dsLON[locc])+', '+str(dsLAT[locc])\n",
    "# ax.set_title(strng)\n",
    "\n",
    "ax.set_ylim([0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
