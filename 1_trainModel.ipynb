{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f5c26-4fa7-4f04-9a5e-573e97efb320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras.layers import PReLU\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import random\n",
    "from itertools import combinations as cb\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5a871-e40e-46d0-97ce-bb3512df65cd",
   "metadata": {},
   "source": [
    "### Define user parameters and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff2002-c34e-4259-b99e-eb79da02d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years to train over\n",
    "training_years = np.arange(2004,2020)\n",
    "\n",
    "# directory containing the tiled training data\n",
    "data_direc = '/direcc/jpflug/ML_layers/WUS_tiles/'\n",
    "\n",
    "# number of days in the data year\n",
    "# keep this lower than 360\n",
    "noDays = 330\n",
    "\n",
    "# specify where to output the trained model outputs\n",
    "modelOutputs_direc = data_direc+'temp/trial8/'\n",
    "\n",
    "# specify the number of epochs to train the model over\n",
    "no_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0d1a1-f4d9-4aa5-9bb7-335b9576bcf8",
   "metadata": {},
   "source": [
    "### General script functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ecea0-9cfd-487b-b482-a9e2a904457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWE melt-correction function\n",
    "def SWE_adjust(SWE_array,SCF_array):\n",
    "    snowArr_out = SWE_array.copy()\n",
    "    for i in range(snowArr_out.shape[1]):\n",
    "        A = SWE_array[:,i].copy()\n",
    "        A_max = np.max(A)\n",
    "        A_loc = np.where(A == A_max)[0][0]\n",
    "        A_orig = A[:A_loc]\n",
    "        B = SCF_array[:,i].copy()\n",
    "        idx = np.where(B == np.max(B))[0][0]\n",
    "        B[:idx+1] = np.nan\n",
    "        idx = np.where(B == 0)[0]\n",
    "        if len(idx) > 0:\n",
    "            idx = idx[0]\n",
    "            if idx < A_loc:\n",
    "                break\n",
    "            else:\n",
    "                A = A - A[idx]\n",
    "                A[A < 0] = 0\n",
    "                snowArr_out[:,i] = A\n",
    "                if np.nanmax(snowArr_out) <= 0:\n",
    "                    snowArr_out[:,i] = SWE_array[:,i]\n",
    "                else:\n",
    "                    pctDiff = A_max/np.nanmax(snowArr_out[:,i])\n",
    "                    snowArr_out[A_loc:,i] = snowArr_out[A_loc:,i]*pctDiff\n",
    "                    snowArr_out[:A_loc,i] = A_orig\n",
    "    return snowArr_out\n",
    "\n",
    "# fill no data and sporadic snow presence after snow disappearance\n",
    "def postProcess_SCA(snowCov_array):\n",
    "    # loop through the cells\n",
    "    snowCov_out = snowCov_array.copy()\n",
    "    for i in range(snowCov_array.shape[1]):\n",
    "        A = snowCov_array[:,i].copy()\n",
    "        # find valid values, and their indices\n",
    "        mask = A > 0.01\n",
    "        indices = np.where(mask)[0]\n",
    "        # loop through the valid values\n",
    "        for idx in indices:\n",
    "            # make sure this value is not at the beginning or end of the array\n",
    "            if idx > 0 and idx < len(A) - 1:\n",
    "                try:\n",
    "                    # find preceding valid value\n",
    "                    starter = idx - 1\n",
    "                    while A[starter] < 0:\n",
    "                        starter = starter - 1\n",
    "                    # find following valid value\n",
    "                    ender = idx + 1\n",
    "                    while A[ender] < 0:\n",
    "                        ender = ender + 1\n",
    "                    # print(starter,ender)\n",
    "                    if (A[starter] <= 0.01) & (A[ender] <= 0.01):\n",
    "                        A[idx] = 0\n",
    "                except:\n",
    "                    print(i,'running against the array boundary')\n",
    "        # find zero values, and their indices\n",
    "        mask = A == 0\n",
    "        indices = np.where(mask)[0]\n",
    "        # loop through the valid values\n",
    "        for idx in indices:\n",
    "            # make sure this value is not at the beginning or end of the array\n",
    "            if idx > 0 and idx < len(A) - 1:\n",
    "                try:\n",
    "                    # find preceding valid value\n",
    "                    starter = idx - 1\n",
    "                    while A[starter] < 0:\n",
    "                        starter = starter - 1\n",
    "                    # find following valid value\n",
    "                    ender = idx + 1\n",
    "                    while A[ender] < 0:\n",
    "                        ender = ender + 1\n",
    "                    # print(starter,ender)\n",
    "                    if (A[starter] > 0.01) & (A[ender] > 0.01):\n",
    "                        A[idx] = -10000\n",
    "                except:\n",
    "                    continue          \n",
    "        # Replace no-data values with zero from the start to the first positive index\n",
    "        first_positive_index = np.argmax(A > 0.05)\n",
    "        A[:first_positive_index][A[:first_positive_index] == -10000] = 0\n",
    "        # Replace no-data values with zero from the last positive index to the end\n",
    "        last_positive_index = len(A) - np.argmax(A[::-1] > 0.05) - 1\n",
    "        A[last_positive_index + 1:][A[last_positive_index + 1:] == -10000] = 0\n",
    "        snowCov_out[:,i] = A\n",
    "        if np.mod(i,1000) == 0:\n",
    "            print(i)\n",
    "    return snowCov_out\n",
    "\n",
    "# save/store custom loss functions\n",
    "# for now, keeping only what was used in Pflug et al. (202X)\n",
    "def default_mean_squared_error(y_true, y_pred):\n",
    "    loss = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30b65c-1071-444a-8479-6c0a4a296891",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc245a-313a-4ead-b162-5b0ec891e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowLSTM_linear_zeroBound():\n",
    "    # model input layers\n",
    "    input_1 = Input(shape=(noDays, 1)) \n",
    "    # masking layer, so snow cover data\n",
    "    input_1_masked = Masking(mask_value=-10000.0)(input_1)\n",
    "    input_2 = Input(shape=(noDays, 1)) \n",
    "    # masking layer, so snow cover data\n",
    "    input_2_masked = Masking(mask_value=-10000.0)(input_2)\n",
    "    input_3 = Input(shape=(noDays, 1))\n",
    "    input_4 = Input(shape=(noDays, 1))\n",
    "    \n",
    "    # LSTM layer for the first input\n",
    "    lstm_1 = LSTM(units=32)(input_1_masked)\n",
    "    # LSTM layer for the second input\n",
    "    lstm_2 = LSTM(units=32)(input_2_masked)\n",
    "    # LSTM layer for the third input\n",
    "    lstm_3 = LSTM(units=32)(input_3)\n",
    "    # LSTM layer for the third input\n",
    "    lstm_4 = LSTM(units=32)(input_4)\n",
    "    # Concatenate the LSTM layers\n",
    "    concatenated = Concatenate()([lstm_1, lstm_2, lstm_3, lstm_4])\n",
    "    \n",
    "    # Dense layers\n",
    "    output_1 = Dense(units=noDays, activation='linear',name='output_mse1')(concatenated)\n",
    "    # output_2 = Dense(units=noDays, activation='linear',name='output_mse2')(concatenated)\n",
    "    output_3 = Dense(units=noDays, activation='linear',name='output_mse3')(concatenated)\n",
    "    # output_1 = Dense(units=noDays, activation='relu',name='output_mse1')(concatenated)\n",
    "    # output_2 = Dense(units=noDays, activation='relu',name='output_mse2')(concatenated)\n",
    "    \n",
    "    # output_2_zero_bound = CustomOutputLayer(name='custom_output')([output_2, input_2])\n",
    "    \n",
    "    # Create model\n",
    "    # model = Model(inputs=[input_1, input_2, input_3, input_4], outputs=[output_1,output_2_zero_bound,output_3])\n",
    "    model = Model(inputs=[input_1, input_2, input_3, input_4], outputs=[output_1,output_3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4df763-4c3a-4de4-aac2-dd06bd07a6d8",
   "metadata": {},
   "source": [
    "### Load the model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7d942-750a-4c7c-9681-49d3933eea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate load\n",
    "dsSWE = np.load(modelOutputs_direc+'dsSWE_norm.npy')\n",
    "dsSCF = np.load(modelOutputs_direc+'dsSCF_norm.npy')\n",
    "SCFaccum = np.load(modelOutputs_direc+'dsSCFaccum_norm.npy')\n",
    "dsT = np.load(modelOutputs_direc+'dsT_norm.npy')\n",
    "dsP = np.load(modelOutputs_direc+'dsP_norm.npy')\n",
    "print(dsSWE.shape)\n",
    "\n",
    "# load the pre-defined random folds\n",
    "five_split = np.load(modelOutputs_direc+'FOLDSidxs.npy')\n",
    "# determine combinations of the five folds\n",
    "comb = cb([0,1,2,3,4], 4)\n",
    "combines = list(comb)\n",
    "\n",
    "# sanity check plotting\n",
    "locc = random.randint(0,SCFaccum.shape[1])\n",
    "fg,ax = plt.subplots()\n",
    "ax.plot(np.arange(noDays),dsSWE[:,locc],'-k')\n",
    "ax.scatter(np.arange(noDays),dsSCF[:,locc])\n",
    "ax.scatter(np.arange(noDays),SCFaccum[:,locc])\n",
    "ax.plot(np.arange(noDays),dsT[:,locc])\n",
    "ax.plot(np.arange(noDays),dsP[:,locc])\n",
    "ax.set_ylim([0,1])\n",
    "\n",
    "# make sure models arent being carried from previous iterations\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    print('no model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74625eee-02dc-4fe2-acde-0ccb3970a3ba",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660293b-c161-4208-8021-559b2b88ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the combinations\n",
    "for i in range(4,len(combines)):\n",
    "    # training indices\n",
    "    train_list = np.concatenate(list(five_split[np.array(combines[i])]))\n",
    "    # remaining indices\n",
    "    nontraingp = np.setxor1d(combines[i], [0,1,2,3,4])\n",
    "    # split the remaining indices into excluded/validation datasets\n",
    "    excluded_list = five_split[nontraingp[0]][0:len(five_split[nontraingp[0]])//2]\n",
    "    valid_list = five_split[nontraingp[0]][len(five_split[nontraingp[0]])//2 : len(five_split[nontraingp[0]])]\n",
    "    \n",
    "    # filepaths and checkpoints to save the model output to\n",
    "    filepath_1 = modelOutputs_direc+'linear_bestoutput'+ str(i) + \"fold\" +'.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath_1, \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True,\n",
    "                                 mode='min')\n",
    "    callbacks = [checkpoint]\n",
    "    \n",
    "    # pull in the model\n",
    "    model = snowLSTM_linear_zeroBound()\n",
    "    \n",
    "    # perform the post-processing corrections to the snow cover data for sporadic snow presence and zeros at the beginning/end\n",
    "    Barray = postProcess_SCA(SCFaccum[:,train_list])\n",
    "    Carray = postProcess_SCA(SCFaccum[:,valid_list])\n",
    "    Darray_SWE = SWE_adjust(dsSWE[:,train_list],Barray)\n",
    "    Darray_SWE[Barray == 0] = 0\n",
    "    Earray_SWE = SWE_adjust(dsSWE[:,valid_list],Carray)\n",
    "    Earray_SWE[Carray == 0] = 0\n",
    "    \n",
    "    # run the training    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'output_mse1': default_mean_squared_error, 'output_mse3': default_mean_squared_error},\n",
    "                  loss_weights={'output_mse1': 1.0, 'output_mse3': 1.0})\n",
    "    model.fit([dsSCF[:,train_list].T,Barray.T,dsT[:,train_list].T,dsP[:,train_list].T],\n",
    "              [dsSWE[:,train_list].T,Darray_SWE.T],epochs=no_epochs,batch_size=64,\n",
    "              validation_data=([dsSCF[:,valid_list].T,Carray.T,dsT[:,valid_list].T,dsP[:,valid_list].T],\n",
    "                               [dsSWE[:,valid_list].T,Earray_SWE.T]),callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
